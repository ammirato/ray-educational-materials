{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Ray RLlib\n",
    "\n",
    "<img src=\"../_static/assets/Generic/ray_logo.png\" width=\"20%\" loading=\"lazy\">\n",
    "\n",
    "## About this notebook\n",
    "\n",
    "### Is it right for you?\n",
    "\n",
    "This notebook is an example-based introduction to the Ray RLlib.\n",
    "\n",
    "You will go through an end-to-end example that covers data loading, training, hyper-parameter tuning, predicting and serving. Along the way you will learn about Ray AIR's specialized libraries that collectively form a unified API for scalable ML applications.\n",
    "\n",
    "It is right for you if:\n",
    "\n",
    "* have basic familiarity with Ray project\n",
    "* you want to learn about Ray AIR: the unified API for scalable ML applications\n",
    "* you have an existing ML application or workload and you look for tools that will let you scale it easily.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "For this notebook you should have:\n",
    "\n",
    "* practical Python and machine learning experience\n",
    "\n",
    "You have completed:\n",
    "* [Overview of Ray](https://github.com/ray-project/ray-educational-materials/blob/main/Introductory_modules/Overview_of_Ray.ipynb)\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "Upon completion of this notebook, you will know about:\n",
    "\n",
    "* high-level ML libraries that compose Ray AIR: Data, Train, Tune, Serve, and RLlib\n",
    "* how to use Ray AIR as a unified toolkit to write an end-to-end ML application in Python as well as scale individual jobs\n",
    "* problems and challenges that Ray AIR attempt to solve\n",
    "\n",
    "### What will you do?\n",
    "\n",
    "You will also scale reinforcement learning (RL) application with RLlib and practice key concepts relevant in the RL domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Overview of Ray AI Runtime (AIR)\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong><a href=\"https://docs.ray.io/en/latest/ray-air/getting-started.html\" target=\"_blank\">Ray AI Runtime (AIR)</a></strong> is an open-source, Python, domain specific library that equips ML engineers, data scientists, and researchers with a scalable and unified toolkit for ML applications.\n",
    "</div>\n",
    "\n",
    "Ray AIR is built on top of Ray core. It caters for distributed data processing, model training, tuning, model serving, and reinforcement learning, all in Python. To that end it enables both individual workloads and end-to-end use cases to be implemented in the single unified library.\n",
    "\n",
    "### Machine learning workflow with Ray AIR\n",
    "\n",
    "Each of the five native libraries that Ray AIR wraps is focused on a specific stage of the ML workflow. Because this abstraction layer is built on top of Ray Core, it is distributed and scalable. Ray AIR brings together an ever-growing ecosystem of integrations with your favorite machine learning frameworks.\n",
    "\n",
    "|<img src=\"../_static/assets/Introduction_to_Ray_AIR/e2e_air.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Ray AIR enables end-to-end ML development and provides multiple options to integrate with other tools and libraries form the MLOps ecosystem.|\n",
    "\n",
    "1. [Ray Data](https://docs.ray.io/en/latest/data/dataset.html): scalable, framework-agnostic loading and transforming raw data\n",
    "1. [Ray Train](https://docs.ray.io/en/latest/train/train.html): distributed multi-node and multi-core model training with fault tolerance that integrates with your favorite training libraries\n",
    "1. [Ray Tune](https://docs.ray.io/en/latest/tune/index.html): scales experiment execution and hyper-parameter tuning to optimize model performance\n",
    "1. [Ray Serve](https://docs.ray.io/en/latest/serve/index.html): deploys your model for online or batch inference\n",
    "1. [Ray RLlib](https://docs.ray.io/en/latest/rllib/index.html): distributed reinforcement learning workloads that integrate with the other Ray AIR libraries above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Production ready reinforcement learning with RLlib\n",
    "\n",
    "In addition to scaling end-to-end workflows with supervised learning problems, we can use Ray AIR to scale reinforcement learning workloads. Here, we will demonstrate this by training a reinforcement learning agent using online training.\n",
    "\n",
    "**A Brief Primer on Reinforcement Learning Basics**\n",
    "\n",
    "Reinforcement learning (RL) involves an **agent** learning what to do through **rewards** based on its interactions from its **environment**. Unlike other types of machine learning, the path to maximize rewards is not prescribed, but rather must be learned through trying and feedback over time. To unpack this further, here are some key componenents of RL problem:\n",
    "\n",
    "- **Action Space** - all possible actions; could be discrete steps (left, right) or continuous (accelerate $F  \\frac{m}{s^2}$)\n",
    "- **State Space** - a complete description of the environment; a *value function* specifies the value of reward the agent can accumulate in the future starting from that state\n",
    "- **Observation Space** - an observation by the agent of certain parts of the state\n",
    "- **Reward** - feedback, positive or negative, after each action; defines the goal\n",
    "- **Policy** - defines the learning agent's way of behaving based on its expected sum over all future rewards\n",
    "\n",
    "![Agent/Env](https://www.kdnuggets.com/images/reinforcement-learning-fig1-700.jpg)\n",
    "\n",
    "We'll go into how to work with these components in the coding exercise. For now, let's chat about how to run reinforcement learning applications with Ray RLlib.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ray RLLib\n",
    "***\n",
    "\n",
    "![RLlib Highlight](../_static/assets/Introduction_to_Ray_AIR/rllib_highlight.png)\n",
    "\n",
    "RLlib is an open-source library for reinforcement learning (RL), offering support for [production-level](https://www.anyscale.com/events/2021/06/23/applying-ray-and-rllib-to-real-life-industrial-use-cases), distributed RL workloads while maintaining unified and simple APIs for a large variety of industry applications. As part of the Ray ecosystem, RLlib integrates well with other Ray libraries like Ray Tune for checkpointing and Ray Serve for deploying models.\n",
    "\n",
    "**Some Key Features of RLlib:**\n",
    "\n",
    "- **[PyTorch](https://github.com/ray-project/ray/blob/master/rllib/examples/custom_torch_policy.py) and [Tensorflow](https://github.com/ray-project/ray/blob/master/rllib/examples/custom_tf_policy.py)** - available as backends, with the option to switch between them with one line of code\n",
    "- **Hightly Distributed** - inherits from Ray Core and allows you to configure `num_workers` to run on hundreds of nodes\n",
    "- **Vectorized and Remote Environments** - batched and parallel environments that auto-vectorizes `gym.Envs` via the `num_envs_per_worker` config\n",
    "- **Support for Multi-Agent** - convert custom `gym.Envs` into a multi-agent set-up to start training with cooperative policies, adversarial scenarios, and/or independent learning\n",
    "- **External Simulators** - support for external environment API and comes with a pluggable, off-the-shelf client/server setup that allows you to run hundreds of independent simulators on the \"outside\" connecting to a central RLlib Policy-Server that learns and serves actions.\n",
    "- **Offline Support** - comes with several offline algorithms (CQL, MARWIL, and DQfD) allowing either behavior-cloning an existing system or learning how to improve it\n",
    "- [**Algorithms**](https://docs.ray.io/en/latest/rllib/rllib-algorithms.html) - a growing collection of 25+ algorithms to apply in offline, model-free on-policy, model-free off-policy, model-based, derivative-free, recommender systems, contextual bandits, multi-agent, and other RL\n",
    "- [**Environments**](https://docs.ray.io/en/latest/rllib/rllib-env.html) - support for several different types of environments including OpenAI Gym, user-defined, multi-agent, and batch environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: CartPole Training and Online Evaluation\n",
    "\n",
    "For our example, we will run training on the [CartPole environment](https://www.gymlibrary.dev/environments/classic_control/cart_pole/) from [OpenAI Gym](https://www.gymlibrary.dev/). The premise is essentially that there is a pole attached to a cart on a frictionless track and the agent's job is to balance this pendulum upright by moving left and right. The observation space consists of the cart position, cart velocity, pole angle, and pole angular velocity, and the goal is to keep the pole upright for as long as possible.\n",
    "\n",
    "![Cartpole](https://www.gymlibrary.dev/_images/cart_pole.gif)\n",
    "\n",
    "*Figure 8*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Relevant Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from ray.air import RunConfig\n",
    "from ray.air import ScalingConfig\n",
    "\n",
    "from ray.air import Checkpoint\n",
    "from ray.air import Result\n",
    "\n",
    "from ray.train.rl import RLTrainer\n",
    "from ray.train.rl import RLPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. To begin, we'll be using [OpenAI Gym](https://www.gymlibrary.dev/) which is a standard open source Python library for developing and comparing reinforcement learning algorithms as well as providing a standard set of environments.\n",
    "2. With Ray AIR's `RunConfig` and `ScalingConfig` we can specify configurations for training/tuning runs and scaling training respectively so that your settings are preserved in the pipeline.\n",
    "3. Callback to `Checkpoint` and `Result` objects from previous sections that store the state of your model and the result from a training or tuning trial.\n",
    "4. Import RL specific trainers and predictors which are able to take in Ray datasets and preprocessors from prior steps.\n",
    "\n",
    "Note: We are using a Ray AIR wrapper for RLlib's trainable which allows a smoother integration with the Ray ecosystem. For custom environments, preprocessors, or models, you can check out the [Training APIs for Rllib](https://docs.ray.io/en/latest/rllib/rllib-training.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rl(num_workers, use_gpu):\n",
    "    trainer = RLTrainer(\n",
    "        run_config=RunConfig(stop={\"training_iteration\": 5}),\n",
    "        scaling_config=ScalingConfig(num_workers=num_workers, use_gpu=use_gpu),\n",
    "        algorithm=\"PPO\",\n",
    "        config={\n",
    "            \"env\": \"CartPole-v0\",\n",
    "            \"framework\": \"torch\",\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    return trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a training function, `train_rl()` that takes in:\n",
    "- `num_workers`: int, the number of workers to start\n",
    "- `use_gpu`: bool, whether to use gpu\n",
    "\n",
    "and creates an `RLTrainer`, similar to the `Trainer` object we encountered in the Ray Train section, which specifies:\n",
    "\n",
    "- `RunConfig`: sets up how the training run should happen\n",
    "- `ScalingConfig`: allows you to adjust settings for how to scale training\n",
    "- `algorithm`: we use the [`PPO` algorithm](https://openai.com/blog/openai-baselines-ppo/)\n",
    "- `config`: specifies our `CartPole-v1` environment and uses Tensorflow as a our backend\n",
    "\n",
    "When we call `train_rl()`, it returns a `Result` object automatically created by `trainer.fit()` through which we can access a `Checkpoint` of the trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define an Evaluation Function\n",
    "\n",
    "Next, we want to create a function to evaluate how well our model trained. It's performance will be evaluated on a reset version of the same environment. In this online evaluation technique, unlike supervised learning cases where we evaluate on a static test set, we probe how well the agent performs through a live simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(checkpoint, num_episodes):\n",
    "    predictor = RLPredictor.from_checkpoint(checkpoint)\n",
    "\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "    rewards = []\n",
    "    for i in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        reward = 0.0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = predictor.predict(np.array([obs]))\n",
    "            obs, r, done, _ = env.step(action[0])\n",
    "            reward += r\n",
    "        rewards.append(reward)\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an `evaluate()` function that takes in:\n",
    "- `checkpoint`: the saved model state from the training `Result`\n",
    "- `num_episodes`: the number of episodes to run, i.e. agent-environment iteration cycles\n",
    "\n",
    "To begin, we:\n",
    "\n",
    "- Create an `RLPredictor` from the `checkpoint`\n",
    "- Set the environment to `CartPole-v1`\n",
    "- Create a list of rewards for each episode\n",
    "\n",
    "For every episode:\n",
    "- `env.reset()` - the observation state is reset to a uniformly random value `(-0.05, 0.05)` for cart position, cart velocity, pole angle, and pole angular velocity\n",
    "- set `reward` to 0 and `done` flag to `False`\n",
    "\n",
    "While we're not in a terminal state:\n",
    "- take an action based on the trained model's best judgement of the observation space\n",
    "- obtain a new observation state, reward, and done flag after taking a step\n",
    "- we assign a reward of `+1` for every step taken, including the termination step\n",
    "\n",
    "<!-- **Example Terminal States**\n",
    "\n",
    "1. Termination: Pole Angle is greater than $\\pm 12 \\degree$\n",
    "2. Termination: Cart Position is greater than $\\pm2.4$ (center of the cart reaches the edge of the display)\n",
    "3. Truncation: Episode length is greater than 500 (200 for v0) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Online Reinforcement Learning\n",
    "\n",
    "Finally, let's put it all together to train the model, evaluate the policy on a fresh environment (using the checkpoint from training) for `num_episodes`. For `CartPole-v1`, the reward threshold is set to `+475`, so let's see how we stack up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = train_rl(num_workers=4, use_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = evaluate(result.checkpoint, num_episodes=num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average Reward Over {num_episodes} Episodes: \" f\"{np.mean(rewards)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Exercise**\n",
    "\n",
    "We have mostly kept this example to focus on RLlib, but we can definitely apply our learnings from previous sections to extend this solution. RLlib integrates particularly well with Ray Tune.\n",
    "\n",
    "Modify the function `train_rl(num_workers, use_gpu)` we created above to include a tuning step. Some things that will help you along the way:\n",
    "\n",
    "- RLlib Trainers can be passed into the first argument when instantiating a `Tuner` object.\n",
    "- We want a `Checkpoint` at the end of tuning to access later on in our evaluation step, so turn this parameter to `True`\n",
    "- Remember that Tuner returns a `ResultGrid` that contains all the results from your training run. You can either elect to return the first result, or better yet, return the best result by querying `result_grid.get_best_result()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "\n",
    "def train_rl(num_workers, use_gpu):\n",
    "    trainer = RLTrainer(\n",
    "        run_config=RunConfig(stop={\"training_iteration\": 5}),\n",
    "        scaling_config=ScalingConfig(num_workers=num_workers, use_gpu=use_gpu),\n",
    "        algorithm=\"PPO\",\n",
    "        config={\n",
    "            \"env\": \"CartPole-v1\",\n",
    "            \"framework\": \"torch\",\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    ### MODIFY TRAIN_RL HERE ###\n",
    "\n",
    "    return trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAMPLE IMPLEMENTATION ###\n",
    "\n",
    "from ray import tune\n",
    "\n",
    "def train_rl(num_workers, use_gpu):\n",
    "    trainer = RLTrainer(\n",
    "        run_config=RunConfig(stop={\"training_iteration\": 5}),\n",
    "        scaling_config=ScalingConfig(num_workers=num_workers, use_gpu=use_gpu),\n",
    "        algorithm=\"PPO\",\n",
    "        config={\n",
    "            \"env\": \"CartPole-v1\",\n",
    "            \"framework\": \"torch\",\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    tuner = Tuner(\n",
    "        trainer,\n",
    "        _tuner_kwargs={\"checkpoint_at_end\": True},\n",
    "    )\n",
    "    result = tuner.fit()[0]\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Summary\n",
    "\n",
    "In this section, we trained a reinforcement learning agent using online training in a Cartpole environment.\n",
    "\n",
    "#### Key Concepts\n",
    "\n",
    "Reinforcement Learning Concepts\n",
    "- Agent\n",
    "- Action Space\n",
    "- State Space\n",
    "- Observation Space\n",
    "- Reward\n",
    "- Policy\n",
    "\n",
    "#### Key RLlib Objects\n",
    "\n",
    "* `RunConfig`\n",
    "* `ScalingConfig`\n",
    "* `Checkpoint`\n",
    "* `Result`\n",
    "* `RLTrainer`\n",
    "* `RLPredictor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Resources\n",
    "\n",
    "If you would like to practice your new skills further with some in-depth examples beyond the embedded coding exercises, take a look at this list of suggested problems:\n",
    "\n",
    "* Watch the Ray Summit Talk on [Introduction to Ray AIR](https://github.com/ray-project/hackathon5-algo)\n",
    "* Check out the [Ray AIR Documentation](https://docs.ray.io/en/latest/ray-air/getting-started.html)\n",
    "* Understand its [Components and APIs](https://docs.ray.io/en/latest/ray-air/package-ref.html)\n",
    "* Ray AIR [User Guides](https://docs.ray.io/en/latest/ray-air/user-guides.html) and [Examples](https://docs.ray.io/en/latest/ray-air/examples/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect with the Ray community\n",
    "\n",
    "You can learn and get more involved with the Ray community of developers and researchers:\n",
    "\n",
    "* [Ray documentation](https://docs.ray.io/en/latest)\n",
    "* [Official Ray Website](https://www.ray.io/): Browse the ecosystem and use this site as a hub to get the information that you need to get going and building with Ray.\n",
    "* [Join the Community on Slack](https://forms.gle/9TSdDYUgxYs8SA9e8): Find friends to discuss your new learnings in our Slack space.\n",
    "* [Use the Discussion Board](https://discuss.ray.io/): Ask questions, follow topics, and view announcements on this community forum.\n",
    "* [Join a Meetup Group](https://www.meetup.com/Bay-Area-Ray-Meetup/): Tune in on meet-ups to listen to compelling talks, get to know other users, and meet the team behind Ray.\n",
    "* [Open an Issue](https://github.com/ray-project/ray/issues/new/choose): Ray is constantly evolving to improve developer experience. Submit feature requests, bug-reports, and get help via GitHub issues.\n",
    "* [Become a Ray contributor](https://docs.ray.io/en/latest/ray-contribute/getting-involved.html): We welcome community contributions to improve our documentation and Ray framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../_static/assets/Generic/ray_logo.png\" width=\"20%\" loading=\"lazy\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "567405a8058597909526349386224fe35dd047505a91307e44ed44be00113429"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
