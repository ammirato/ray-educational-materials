{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d64cf3e6-b96f-41f9-9ed2-5ae0c0ecdc00",
   "metadata": {},
   "source": [
    "# Distributed Data Ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37be9631-22cf-462a-a647-14c4feed4b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import data\n",
    "\n",
    "# Step 1: read 100 files in parallel from S3 directory\n",
    "dataset = data.read_csv(paths=\"s3://structured/data\", parallelism=100)\n",
    "\n",
    "# Step 2: partition the dataset into blocks\n",
    "dataset = dataset.repartition(num_blocks=1000)\n",
    "\n",
    "# Step 3: preprocess the data at scale, 1000 blocks in parallel\n",
    "preprocessor = data.preprocessors.StandardScaler(columns=[\"value\"])\n",
    "dataset_transformed = preprocessor.fit_transform(dataset=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ef6443-f57d-4aa7-8b6b-89e48786a69b",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c73786-8964-47aa-9808-7785e2e426bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.air.config import ScalingConfig\n",
    "from ray.train.torch import TorchTrainer\n",
    "\n",
    "# Step 1: setup PyTorch model training as you normally would\n",
    "def train_loop_per_worker():\n",
    "    model = ...\n",
    "    train_dataset = ...\n",
    "    for epoch in range(num_epochs):\n",
    "        ...  # model training logic\n",
    "\n",
    "# Step 2: setup Ray's PyTorch Trainer to run on 32 GPUs\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_per_worker,\n",
    "    scaling_config=ScalingConfig(num_workers=32, use_gpu=True),\n",
    "    datasets={\"train\": train_dataset},\n",
    ")\n",
    "\n",
    "# Step 3: run distributed model training on 32 GPUs\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08884cd7-1770-4c15-8d60-d4d71b0ef87b",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb41e5a2-f5c6-48a8-bda5-1c6b64169727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from ray.air.config import ScalingConfig\n",
    "from ray.train.lightgbm import LightGBMTrainer\n",
    "\n",
    "train_dataset, eval_dataset = ...\n",
    "\n",
    "# Step 1: setup Ray's LightGBM Trainer to train on 64 CPUs\n",
    "trainer = LightGBMTrainer(\n",
    "    ...\n",
    "    scaling_config=ScalingConfig(num_workers=64),\n",
    "    datasets={\"train\": train_dataset, \"eval\": eval_dataset},\n",
    ")\n",
    "\n",
    "# Step 2: setup Ray Tuner to run 1000 trials\n",
    "tuner = tune.Tuner(\n",
    "    trainer=trainer,\n",
    "    param_space=hyper_param_space,\n",
    "    tune_config=tune.TuneConfig(num_samples=1000),\n",
    ")\n",
    "\n",
    "# Step 3: run distributed HPO with 1000 trials; each trial runs on 64 CPUs\n",
    "result_grid = tuner.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaea1c99-b114-4ff4-8641-b8d5c7df3967",
   "metadata": {},
   "source": [
    "# Batch inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ee0e09-d507-47dc-8db7-f1c954e33ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.batch_predictor import BatchPredictor\n",
    "from ray.train.torch import TorchPredictor\n",
    "\n",
    "dataset = ...\n",
    "\n",
    "# Step 1: create batch predictor to run inference at scale\n",
    "batch_predictor = BatchPredictor.from_checkpoint(\n",
    "    checkpoint=model_checkpoint, predictor_cls=TorchPredictor\n",
    ")\n",
    "\n",
    "# Step 2: run batch inference on 64 GPUs\n",
    "results = batch_predictor.predict(dataset, batch_size=512, num_gpus_per_worker=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e07552-8060-420f-9d03-fa33fa943b38",
   "metadata": {},
   "source": [
    "# Model Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98a60a1-0961-450b-8805-2e8dcd2f3e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import serve\n",
    "from ray.serve import PredictorDeployment\n",
    "from ray.train.lightgbm import LightGBMPredictor\n",
    "\n",
    "# Deploy 50 replicas of the LightGBM model as a live endpoint.\n",
    "# Convert incoming JSON requests into a DataFrame.\n",
    "serve.run(\n",
    "    PredictorDeployment.options(\n",
    "        name=\"LightGBM_Service\",\n",
    "        num_replicas=50,\n",
    "    ).bind(\n",
    "        predictor_cls=LightGBMPredictor,\n",
    "        checkpoint=lgbm_best_checkpoint,\n",
    "        http_adapter=serve.http_adapters.pandas_read_json,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aeb7bd3-e7d8-4acb-aea6-51551795b0c4",
   "metadata": {},
   "source": [
    "# Reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6980348d-2e96-4438-8fce-059ebe7873d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "# Step 1: configure PPO to run 64 parallel workers to collect samples from the env.\n",
    "ppo_config = (\n",
    "    PPOConfig()\n",
    "    .environment(env=\"Taxi-v3\")\n",
    "    .rollouts(num_rollout_workers=64)\n",
    "    .framework(\"torch\")\n",
    "    .training(model=rnn_lage)\n",
    ")\n",
    "\n",
    "# Step 2: build the PPO algorithm\n",
    "ppo_algo = ppo_config.build()\n",
    "\n",
    "# Step 3: train and evaluate PPO\n",
    "for _ in range(5):\n",
    "    print(ppo_algo.train())\n",
    "\n",
    "ppo_algo.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
