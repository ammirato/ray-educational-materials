{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Ray AI Runtime (AIR) - Data and Train\n",
    "\n",
    "<img src=\"../_static/assets/Generic/ray_logo.png\" width=\"20%\" loading=\"lazy\">\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong><a href=\"https://docs.ray.io/en/latest/ray-air/getting-started.html\" target=\"_blank\">Ray AI Runtime (AIR)</a></strong> is an open-source, Python-based, domain-specific library that equips ML engineers, data scientists, and researchers with a scalable and unified toolkit for ML applications.\n",
    "</div>\n",
    "\n",
    "Built on top of Ray Core, Ray AIR inherits all the performance and scalability benefits offered by Core while providing a convenient abstraction layer for machine learning. Ray AIR's Python-first native libraries allow ML practitioners to distribute individual workloads, end-to-end applications, and build custom use cases in a unified framework.\n",
    "\n",
    "### Machine learning workflow with Ray AIR\n",
    "\n",
    "|<img src=\"../_static/assets/Introduction_to_Ray_AIR/e2e_air.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Ray AIR enables end-to-end ML development and provides multiple options for integrating with other tools and libraries form the MLOps ecosystem.|\n",
    "\n",
    "### Data loading and model training with Ray AI Runtime\n",
    "\n",
    "To illustrate Ray AIR's capabilities, you will implement an data loading and transforming as well as model training. end-to-end machine learning application that predicts big tips using New York City taxi data. Each section will introduce the relevant Ray AIR library or component and demonstrating its functionality through code examples.\n",
    "\n",
    "|Ray AIR Component|NYC Taxi Use Case|\n",
    "|:--|:--|\n",
    "|Ray Data|Use `Preprocessor` to load and transform input data.|\n",
    "|Ray Train|Use `Trainer` to scale XGBoost model training.|\n",
    "\n",
    "For this classification task, you will apply a simple [XGBoost](https://xgboost.readthedocs.io/en/stable/) (a gradient boosted trees framework) model to the June 2021 [New York City Taxi & Limousine Commission's Trip Record Data](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page). This dataset contains over 2 million samples of yellow cab rides, and the goal is to predict whether a trip will result in a tip greater than 20% or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Ray Data\n",
    "---\n",
    "\n",
    "|<img src=\"../_static/assets/Introduction_to_Ray_AIR/data_highlight.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Ray AIR wraps Ray Data to provide distributed data ingestion and transformation during training, tuning, and inference.|\n",
    "\n",
    "### Introduction to Ray Datasets\n",
    "\n",
    "Backed by PyArrow, [Ray Datasets](https://docs.ray.io/en/latest/data/user-guide.html) parallelize the loading and transforming of data and provide a standard way to pass references to data across Ray libraries and applications. Datasets are not intended to replace more general data processing systems. Instead, it serves as a last-mile bridge from ETL pipeline outputs to distributed applications and libraries in Ray.\n",
    "\n",
    "#### Key features\n",
    "\n",
    "- **Flexibility**\n",
    "\n",
    "    Datasets are compatible with a variety of file formats, data sources, and distributed frameworks. They work seamlessly with library integrations like Dask on Ray and can be passed between Ray tasks and actors without copying data.\n",
    "\n",
    "- **Performance for ML Workloads**\n",
    "\n",
    "    Datasets offer important features like accelerator support, pipelining, and global random shuffles that accelerate ML training and inference workloads. They also support basic distributed data transformations such as map, filter, sort, groupby, and repartition.\n",
    "\n",
    "- **Persistent Preprocessor**\n",
    "\n",
    "    The `Preprocessor` primitive captures and stores the transformations applied to convert inputs into features. It is applied during training, tuning, batch prediction, and serving to keep the preprocessing consistent across the pipeline.\n",
    "    \n",
    "- **Built on Ray Core**\n",
    "\n",
    "    Datasets inherits scalability to hundreds of nodes, efficient memory usage, object spilling, and failure handling from Ray Core. Because Datasets are just lists of object references, they can be passed between tasks and actors without needing to make a copy of the data, which is crucial for making data-intensive applications and libraries scalable.\n",
    "\n",
    "|<img src=\"../_static/assets/Introduction_to_Ray_AIR/data_code.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|A general pattern for creating a `Dataset`, configuring a `Preprocessor`, and passing these into the `Trainer` for consistent data handling throughout the pipeline.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Ray runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start a Ray cluster (see: installation [instructions](https://docs.ray.io/en/latest/ray-overview/installation.html)) so that Ray can utilize all the cores and GPUs available to you as workers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Ray Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Parquet file to Ray Dataset.\n",
    "dataset = ray.data.read_parquet(\n",
    "    \"s3://anyscale-training-data/intro-to-ray-air/nyc_taxi_2021.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation subsets.\n",
    "train_dataset, valid_dataset = dataset.train_test_split(test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split datasets into blocks for parallel preprocessing.\n",
    "# `num_blocks` should be lower than number of cores in the cluster.\n",
    "train_dataset = train_dataset.repartition(num_blocks=5)\n",
    "valid_dataset = valid_dataset.repartition(num_blocks=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common operations on datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many [`Dataset` API elements](https://docs.ray.io/en/latest/data/api/dataset.html#) available for common transformations and operations. Below are few examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect [the schema](https://docs.ray.io/en/latest/data/api/dataset.html#inspecting-metadata) of the underlying Parquet metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Schema of training dataset: \\n {train_dataset.schema()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Count](https://docs.ray.io/en/latest/data/api/dataset.html#inspecting-metadata) the number of rows in the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of samples in training dataset: \\n {train_dataset.count()}\")\n",
    "print(f\"Number of samples in validation dataset: \\n {valid_dataset.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Show](https://docs.ray.io/en/latest/data/api/dataset.html#consuming-datasets) the first five samples from either dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the average `fare_amount` [grouped by](https://docs.ray.io/en/latest/data/api/dataset.html#grouped-and-global-aggregations) `passenger_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.groupby(\"passenger_count\").mean(\"fare_amount\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Split](https://docs.ray.io/en/latest/data/api/dataset.html#ray.data.Dataset.split) dataset into N datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_list = train_dataset.split(n=5)\n",
    "datasets_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the dataset\n",
    "To transform our raw data into features, you will define a `Preprocessor`. [Ray AIR's `Preprocessor`](https://docs.ray.io/en/latest/ray-air/package-ref.html#preprocessor) captures the data transformation you apply and persists:\n",
    "\n",
    "- **During training**\n",
    "\n",
    "    `Preprocessor` is passed into a `Trainer` to `fit` and `transform` input `Datasets`.\n",
    "- **During tuning**\n",
    "\n",
    "    Each `Trial` will create its own copy of the `Preprocessor`, and the fitting and transformation logic will occur once per `Trial`.\n",
    "- **During checkpointing**\n",
    "\n",
    "    The `Preprocessor` is saved in the `Checkpoint` if it was passed into the `Trainer`.\n",
    "- **During predicting**\n",
    "\n",
    "    If the `Checkpoint` contains a `Preprocessor`, then it will be used to call `transform_batch` on input batches prior to performing inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ray AIR provides several [preprocessors out of the box](https://docs.ray.io/en/latest/ray-air/preprocessors.html#) and also supports the implementation of [custom preprocessors](https://docs.ray.io/en/latest/ray-air/preprocessors.html#implementing-custom-preprocessors). Later on, you can compare model performance between the given preprocessor and your custom configuration.\n",
    "\n",
    "Select a [built-in](https://docs.ray.io/en/latest/ray-air/preprocessors.html#types-of-preprocessors) `Preprocessor` and use `fit_transform()` to [apply it](https://docs.ray.io/en/latest/ray-air/package-ref.html#preprocessor) to the dataset. Visualize the results (perhaps using the [integration with pandas](https://docs.ray.io/en/latest/data/api/input_output.html#ray.data.from_pandas) to generate a histogram view)\n",
    "\n",
    "Note: You may want to create a sample dataset to transform, as the original data and preprocessor will be passed to the `Trainer` in the next step for transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.preprocessors import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a preprocessor to normalize the columns by their range.\n",
    "preprocessor = MinMaxScaler(columns=[\"trip_distance\", \"trip_duration\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.preprocessors import PowerTransformer\n",
    "\n",
    "sample_data = ray.data.read_parquet(\n",
    "    \"s3://anyscale-training-data/intro-to-ray-air/nyc_taxi_2021.parquet\"\n",
    ")\n",
    "\n",
    "# create new preprocessor\n",
    "sample_preprocessor = PowerTransformer(\n",
    "    columns=[\"trip_distance\", \"trip_duration\"], power=0.5\n",
    ")\n",
    "\n",
    "# apply the transformation\n",
    "transformed_data = sample_preprocessor.fit_transform(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For numerical features, you can choose an appropriate AIR `Preprocessor` depending on your data's properties:\n",
    "\n",
    "- `PowerTransformer`  \n",
    "Your data isn't normal, but you need it to be.\n",
    "- `Normalizer`  \n",
    "You need unit norm rows.\n",
    "- `MinMaxScaler`  \n",
    "You are unsure of the distribution of your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "#### Key concepts\n",
    "\n",
    "**`Dataset`**\n",
    "\n",
    "The standard way to load and exchange data in Ray AIR. In AIR, Datasets are used extensively for data loading and transformation. They are meant as a last-mile bridge from ETL pipeline outputs to distributed applications and libraries in Ray.\n",
    "\n",
    "**`Preprocessor`**\n",
    "\n",
    "Preprocessors are primitives that transform input data into features. They operate on Datasets, making them scalable and compatible with a variety of datasources and dataframe libraries.\n",
    "\n",
    "Preprocessors persist through various stages of the pipeline:\n",
    "\n",
    "- During training to fit and transform input data\n",
    "- In each trial of hyperparameter tuning\n",
    "- Within a checkpoint\n",
    "- On input batches for inference\n",
    "\n",
    "AIR comes with a collection of built-in preprocessors, and you can also define your own with simple templates (see the [user guide](https://docs.ray.io/en/latest/ray-air/preprocessors.html) for more information)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Ray Train\n",
    "---\n",
    "\n",
    "|<img src=\"../_static/assets/Introduction_to_Ray_AIR/train_highlight.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Ray AIR wraps Ray Train to provide distributed model training.|\n",
    "\n",
    "### Introduction to Ray Train\n",
    "\n",
    "#### Common challenges with training\n",
    "\n",
    "ML pracitioners tend to run into a few common problems with training models that prompt them to consider distributed solutions:\n",
    "\n",
    "1. Training time is [too long](https://www.anyscale.com/blog/how-anastasia-implements-ray-and-anyscale-to-speed-up-ml-processes-9x) to be practical.\n",
    "2. The [data is too large](https://www.anyscale.com/blog/how-ray-and-anyscale-make-it-easy-to-do-massive-scale-machine-learning-on) to fit on one machine.\n",
    "3. [Training many models](https://www.anyscale.com/blog/training-one-million-machine-learning-models-in-record-time-with-ray) sequentially doesn't utilize resources efficiently.\n",
    "4. The [model itself is too large](https://www.uber.com/blog/horovod-ray/) to fit on a single machine.\n",
    "\n",
    "[Ray Train](https://docs.ray.io/en/latest/ray-air/trainer.html) addresses these issues by improving performance through distributed multi-node training.\n",
    "\n",
    "#### Integration with Ray ecosystem\n",
    "\n",
    "Ray Train's `Trainers` integrates well with the rest of the Ray ecosystem:\n",
    "\n",
    "* **Ray Data**  \n",
    "    * Enables scalable data loading and preprocessing with Ray `Datasets` and `Preprocessors`.\n",
    "* **Ray Tune**\n",
    "    * Composes with `Tuners` for distributed hyperparameter tuning.\n",
    "* **Ray AIR Predictor**\n",
    "    * As a checkpointed trained model to be applied during inference.\n",
    "* **Popular ML training frameworks**\n",
    "    * [PyTorch](https://docs.ray.io/en/latest/ray-air/package-ref.html#pytorch)\n",
    "    * [Tensorflow](https://docs.ray.io/en/latest/ray-air/package-ref.html#tensorflow)\n",
    "    * [Horovod](https://docs.ray.io/en/latest/ray-air/package-ref.html#horovod)\n",
    "    * [XGBoost](https://docs.ray.io/en/latest/ray-air/package-ref.html#xgboost)\n",
    "    * [HuggingFace Transformers](https://docs.ray.io/en/latest/ray-air/package-ref.html#huggingface)\n",
    "    * [Scikit-Learn](https://docs.ray.io/en/latest/ray-air/package-ref.html#scikit-learn)\n",
    "    * and more!\n",
    "\n",
    "#### Useful features\n",
    "\n",
    "* Callbacks for early stopping\n",
    "* Checkpointing\n",
    "* Integration with observability tools like Tensorboard, Weights & Biases, and mlflow\n",
    "* Export mechanisms for models\n",
    "\n",
    "|<img src=\"../_static/assets/Introduction_to_Ray_AIR/train_code.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Define the `Trainer` object and then fit it to the training dataset. This snippet uses a `TorchTrainer`, however, this may be swapped out with any [integration](https://docs.ray.io/en/latest/ray-air/package-ref.html#trainer-and-predictor-integrations) or custom-defined `Trainer`.|\n",
    "\n",
    "In the next section, define and fit an XGBoost Trainer to fit the NYC taxi data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define AIR `Trainer`\n",
    "\n",
    "Ray AIR provides a variety of built-in [`Trainers`](https://docs.ray.io/en/latest/ray-air/trainer.html) (PyTorch, Tensorflow, HuggingFace, etc.). In the example below, you will use a Ray `XGBoostTrainer` which [offers support](https://docs.ray.io/en/latest/train/gbdt.html) for XGBoost models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.air.config import ScalingConfig\n",
    "from ray.train.xgboost import XGBoostTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = XGBoostTrainer(\n",
    "    label_column=\"is_big_tip\",\n",
    "    num_boost_round=50,\n",
    "    scaling_config=ScalingConfig(\n",
    "        num_workers=5,\n",
    "        use_gpu=False,\n",
    "    ),\n",
    "    params={\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": [\"logloss\", \"error\"],\n",
    "        \"tree_method\": \"approx\",\n",
    "    },\n",
    "    datasets={\"train\": train_dataset, \"valid\": valid_dataset},\n",
    "    preprocessor=preprocessor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct a `Trainer`, you provide three base components:\n",
    "\n",
    "- A `ScalingConfig` which [specifies](https://docs.ray.io/en/releases-2.0.0rc0/ray-air/config-scaling.html) how many parallel training workers and what type of resources (CPUs/GPUs) to use per worker during training; supports seamless scaling across heterogeneous hardware.\n",
    "- A dictionary of training and validation sets.\n",
    "- The `Preprocessor` used to transform the `Datasets`.\n",
    "\n",
    "Optionally, you can choose to add `resume_from_checkpoint` which allows you to continue training from a [saved checkpoint](https://docs.ray.io/en/latest/ray-air/package-ref.html#ray.air.checkpoint.Checkpoint) should the run be interrupted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke training. \n",
    "# The resulting object grants access to metrics, checkpoints, and errors.\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect training results\n",
    "\n",
    "You can check out the training results from the `Result` object with the following calls:\n",
    "\n",
    "```python\n",
    "# returns last saved checkpoint\n",
    "result.checkpoint\n",
    "\n",
    "# returns the `n` best saved checkpoints as configured in `RunConfig.CheckpointConfig`\n",
    "result.best_checkpoints\n",
    "\n",
    "# returns the final metrics as reported\n",
    "result.metrics\n",
    "\n",
    "# returns an Exception if training failed\n",
    "result.error\n",
    "```\n",
    "\n",
    "Inspect your training result below. What is the reported accuracy for the training and validation runs?\n",
    "\n",
    "Note: `result.error` contains the binary classification error rate in this case calculated as `#(wrong cases)/#(all cases)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Result metrics: \\n {result.metrics} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training accuracy: {1 - result.metrics['train-error']:.4f}\")\n",
    "print(f\"Validation accuracy: {1 - result.metrics['valid-error']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "#### Key concepts\n",
    "\n",
    "**`Checkpoint`**\n",
    "\n",
    "Store the full state of the model periodically, so that partially trained models are available and can be used to resume training from an intermediate point, instead of starting from scratch; also allows for the best model to be saved for batch inference later on.\n",
    "\n",
    "**`Trainer`**\n",
    "\n",
    "Trainers are wrapper classes around third-party training frameworks such as XGBoost, Pytorch, and Tensorflow. They are built to help integrate with Ray Actors (for distribution), Ray Datasets, and Ray Tune."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shutdown Ray runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disconnect the worker, and terminate processes started by ray.init().\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Run data loading and model training as an Anyscale Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../_static/assets/Generic/ray_logo.png\" width=\"20%\" loading=\"lazy\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "94b6e089e9a877ccf5388c6b4ee308fa971004a47acecdf1f32f8ce7a67a52c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
